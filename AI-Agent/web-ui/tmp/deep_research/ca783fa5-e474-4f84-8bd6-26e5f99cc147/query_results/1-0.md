Query: reinforcement learning for large language models history and techniques
Extracted page content:
 Title: Reinforcement Learning in Large Language Models: A Technical Overview
URL Source: https://johndcyber.com/reinforcement-learning-in-large-language-
models-a-technical-overview-40dca7917a0f Published Time:
2024-09-25T16:24:07.897Z Markdown Content: [![Image 1: John D
Cyber](https://miro.medium.com/v2/resize:fill:88:88/1*AgGSx50f8BzuMi-
oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---byline--
40dca7917a0f---------------------------------------) ![Image
2](https://miro.medium.com/v2/resize:fit:700/0*ufT2td3ymj4SxspZ.png)
Introduction ------------ Reinforcement learning (RL) is an area of machine
learning where an agent learns to make decisions by interacting with an
environment. The agent is rewarded for favorable actions and penalized for
unfavorable ones. RL is increasingly being applied to large language models
(LLMs) to improve their efficiency, accuracy, and adaptability in various
tasks.LLMs like GPT-4 have transformed natural language processing (NLP), but
they still face limitations, particularly in decision-making tasks.
Reinforcement learning can help bridge this gap by allowing models to optimize
their behavior based on rewards. Companies like TikTok, Spotify, and Google
are leveraging RL to personalize user experiences, recommending content that
aligns with user preferences.This article provides an in-depth technical
explanation of how reinforcement learning is applied to LLMs, examples of tech
companies utilizing this technique, and steps to deploy RL models on cloud
platforms. ![Image
3](https://miro.medium.com/v2/resize:fit:700/0*37JcuN1PS3S5n02e) Photo by
[Ricardo Gomez
Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) Key
Components of Reinforcement Learning ----------------------------------------
1. Agent: The decision-maker, such as a model that interacts with the
environment. 2. Environment The space where the agent operates. For LLMs, the
environment might be text data or user interactions. 3. State: The current
situation the agent is in. 4. Action: Choices the agent can make at any given
state. 5. Reward: Feedback given to the agent based on its actions. 6. Policy:
The strategy the agent follows to decide its next action. 7. Value Function:
Evaluates the long-term rewards for taking a certain action. Reinforcement
Learning in Large Language Models
----------------------------------------------- ![Image
4](https://miro.medium.com/v2/resize:fit:700/0*Ey7OHIOgVepkOiJu) Photo by
[Google
DeepMind](https://unsplash.com/@googledeepmind?utm_source=medium&utm_medium=referral)
on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
Traditional LLMs generate text based on maximum likelihood estimation (MLE),
which tries to minimize prediction errors. However, this often leads to
suboptimal performance in tasks requiring long-term planning, such as dialogue
systems or content recommendations. RL is introduced in this context to enable
LLMs to optimize their output based on reward feedback, which can be either
human-labeled data or user engagement metrics (clicks, time spent, etc.). How
RL is Applied to LLMs ------------------------- ![Image
5](https://miro.medium.com/v2/resize:fit:700/0*kx0m7hWQH-C3ryiw) Photo by
[Jona](https://unsplash.com/@6690img?utm_source=medium&utm_medium=referral) on
[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) *
Fine-tuning with RL: LLMs are initially trained using unsupervised learning on
large corpora of text. Then, they are fine-tuned using reinforcement learning
to adapt the model to specific tasks like content recommendation, conversation
flow, or code generation. * Reward Models: In applications like [TikTok’s
recommendation system](https://linkedin.com/pulse/ai-behind-tiktoks-addictive-
algorithm-simple-alexander-stahl/) or ChatGPT’s alignment, a reward model is
built that assigns scores to each generated output. The LLM is trained to
maximize these rewards over time, resulting in improved user engagement or
accuracy. * Exploration vs. Exploitation: LLMs face the classic RL dilemma —
whether to explore new potential solutions or exploit known, high-reward
outcomes. This balance is key to generating creative or optimized solutions.
Examples of Reinforcement Learning in Tech
------------------------------------------ 1\\. [TikTok’s Personalized
Recommendations](https://www.linkedin.com/pulse/impact-ai-tiktok-how-
algorithmic-continue-user-experience-lynch-eh5he/) TikTok uses reinforcement
learning to recommend content to users. The app tracks user interactions such
as likes, shares, and watch time, then uses RL to personalize the feed. The
agent (recommendation model) receives a reward for each positive user
interaction and uses that information to curate the next set of content. This
continuous cycle helps TikTok provide increasingly relevant videos to users.
2\\. [Spotify’s Music
Recommendations](https://www.spotify.com/us/safetyandprivacy/understanding-
recommendations) Spotify leverages RL to optimize its Discover Weekly
playlist. The model takes into account the music a user listens to, skips, or
saves, and tries to recommend songs that maximize user satisfaction. The RL
agent receives feedback on the success of its recommendations based on how
much time users spend listening or saving songs to playlists. 3\\. Google
Search Google has incorporated reinforcement learning into its search
algorithms to optimize the search results users are most likely to click on.
The RL agent is rewarded when a user interacts with a search result (e.g.,
clicks, reads) and penalized for poor interactions like rapid bouncing off the
site. Technical Dive: Reinforcement Learning in LLMs
---------------------------------------------- ![Image
6](https://miro.medium.com/v2/resize:fit:700/0*3jNmjK2ITIsTFflQ) Photo by
[Scott
Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral)
on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 1.
Proximal Policy Optimization (PPO) Proximal Policy Optimization (PPO) is one
of the most common RL algorithms used in fine-tuning LLMs. It’s effective for
handling high-dimensional tasks like natural language generation, where the
agent optimizes its policy by interacting with a reward model. In LLMs, PPO is
used for language tasks such as summarization, translation, and dialogue
generation. For example, OpenAI fine-tuned GPT-3 using PPO in reinforcement
learning from human feedback (RLHF), where the agent learned to generate
responses that aligned with human preferences. 2\\. Reinforcement Learning
with Human Feedback (RLHF) RLHF is a specific application of RL where human-
labeled data helps guide the reward model. ChatGPT is a good example of this
approach. The model generates responses, and human annotators provide feedback
on which responses are preferable. Over time, the model learns to optimize its
output to match human preferences more closely. 3\\. Actor-Critic Methods The
Actor-Critic method combines two neural networks: one that takes actions (the
Actor) and one that evaluates the actions (the Critic). The Actor decides what
to do, and the Critic provides feedback on whether that action was good or
bad. In LLMs, this method is used to ensure the model generates high-quality
content that is contextually relevant and engaging. ![Image
7](https://miro.medium.com/v2/resize:fit:700/0*fJUA6ObLgxNv-1nG) Photo by
[Joel Rivera-
Camacho](https://unsplash.com/@actuallyjoel?utm_source=medium&utm_medium=referral)
on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
Deploying Reinforcement Learning Models on Cloud Providers
---------------------------------------------------------- Here’s how you can
deploy your own reinforcement learning models using a cloud provider such as
AWS, Azure, or Google Cloud. Steps to Deploy on AWS ----------------------
**Set up an Environment:**Use Amazon SageMaker to set up your machine learning
environment. SageMaker supports various frameworks for RL, including
TensorFlow and PyTorch.For example, you can use SageMaker RL for training
reinforcement learning models with deep neural networks. **Train the
Model:**Create an RL environment using OpenAI Gym or your custom
environment.Train your model using RL algorithms like PPO, Deep Q-Learning
(DQN), or Actor-Critic. SageMaker provides pre-built algorithms that you can
plug into your training process. **Deploy the Model:**Deploy your model on an
endpoint for inference using SageMaker.Monitor the agent’s performance in
real-time by setting up Amazon CloudWatch to track key metrics like rewards
and losses. Steps to Deploy on Google Cloud -------------------------------
**Use AI Platform:**Google Cloud’s AI Platform supports reinforcement learning
libraries like TF-Agents (built on TensorFlow) and Ray RLlib for scalable
RL.You can set up a training pipeline in Vertex AI and leverage Google
Kubernetes Engine (GKE) for distributed RL training. **Hyperparameter
Tuning:**Google Cloud provides hyperparameter tuning via HyperTune, allowing
you to optimize your RL model’s performance. **Deploy the Model:**After
training, deploy your model using Vertex AI Model service to make it available
for production use. Steps to Deploy on Azure ------------------------ **Azure
Machine Learning:**Use Azure Machine Learning Studio to create, train, and
deploy reinforcement learning models.Azure offers pre-configured environments
for RL training with popular libraries like OpenAI Gym, Ray, and RLlib.
**Distributed Training:**Leverage Azure Kubernetes Service (AKS) for
distributed RL training across multiple agents. **Monitoring and Scaling:**Use
Azure Monitor to track the performance of your model and scale resources up or
down as needed. Summary of Key Points --------------------- Reinforcement
learning (RL) enhances LLMs by allowing them to optimize their behavior based
on rewards, improving long-term decision-making. \\- TikTok, Spotify, and
Google are leveraging RL to enhance user experiences through personalized
recommendations. \\- Common RL algorithms applied to LLMs include PPO, Actor-
Critic methods, and RLHF. \\- RL can be deployed on cloud platforms like AWS,
Google Cloud, and Azure using built-in services and libraries, such as Amazon
SageMaker, Google AI Platform, and Azure Machine Learning. \\- Proximal Policy
Optimization (PPO) and RLHF are key techniques for training RL-enhanced
language models. References ---------- OpenAI: Reinforcement Learning with
Human Feedback. Available at: \\[OpenAI
Blog\\](https://openai.com/research/reinforcement-learning-human-feedback)
TikTok’s Recommendation System: How Reinforcement Learning Drives User
Engagement. Available at:
\\[Medium\\](https://medium.com/@tiktok/recommendation) Spotify’s Discover
Weekly: Leveraging Reinforcement Learning for Personalized Music
Recommendations. Available at: \\[Spotify
Blog\\](https://newsroom.spotify.com/tech) AWS SageMaker RL: Documentation and
Training Guide. Available at: \\[AWS\\](https://aws.amazon.com/sagemaker/)


Extracted page content:
 Title: A Brief History of Large Language Models - DATAVERSITY URL Source:
https://www.dataversity.net/a-brief-history-of-large-language-models/
Published Time: 2023-12-28T08:35:00+00:00 Markdown Content: ![Image
1](https://d3an9kf42ylj3p.cloudfront.net/uploads/2023/12/kf_bhoflargelanguagemodels_dec23-300x224.png)
Large language models are artificial neural networks (algorithms) that have
gone from a recent development to widespread use within a few years. They have
been instrumental in the development of ChatGPT, the next evolutionary step in
artificial intelligence. Generative AI was combined with large language models
to produce a smarter version of artificial intelligence. Large language models
(LLMs) are based on artificial neural networks, and recent improvements in
deep learning have supported their development. A large language model also
uses semantic technology (semantics, the semantic web, and natural language
processes). The history of large language models starts with the concept of
semantics, developed by the French philologist, Michel Bréal, in 1883. Bréal
studied the ways languages are organized, how they change as time passes, and
how words connect within a language. ![Image
2](https://d3an9kf42ylj3p.cloudfront.net/uploads/2024/01/1x1.png)[](https://training.dataversity.net/enterprise?utm_source=dataversity&utm_medium=inline_ad&utm_campaign=DVTC_private_training_temp3&utm_content=copy1)
Currently, semantics is used for languages developed for humans, such as Dutch
or Hindi, and artificial programming languages, such as Python and Java.
Natural language processing, however, is focused on translating human
communications into a language understood by computers, and back again. It
uses systems that can provide an understanding of human instructions, allowing
computers to understand written text, recognize speech, and translate between
computer and human languages. **How Natural Language Processing Was Almost
Lost Before It Started**
--------------------------------------------------------------------- From
1906 to 1912, Ferdinand de Saussure taught Indo-European linguistics, general
linguistics, and Sanskrit at the University of Geneva. During this time he
developed the foundation for a highly functional model of languages as
systems. Then, in 1913, he died, before organizing and publishing his work.
Fortunately, Albert Sechehaye and Charles Bally, two instructors who were also
Saussure’s colleagues, recognized the potential of his concepts and decided
they were important enough to save. The two instructors collected his notes
for his future manuscript, and then made the effort to gather the notes of
Saussure’s students. Based on these, they wrote Saussere’s book, titled _Cours
de Linguistique Générale_ (translated to [_Language as a
Science_](https://ia802607.us.archive.org/8/items/courseingenerall00saus/courseingenerall00saus.pdf),
which eventually evolved into natural language processing (NLP), which was
published in 1916. _Language as a Science_ laid the foundation of the
[structuralist approach](https://anthropology.ua.edu/theory/structuralism/),
and later, natural language processes. **The Need for Language Translation
Jump-Starts Natural Language Processing**
-----------------------------------------------------------------------------
After the end of World War II (1945), the field of [natural language
processing](https://www.dataversity.net/a-brief-history-of-natural-language-
processing-nlp/) received a great deal of attention. Peace talks and the
desire for international trade prompted recognition of the importance of
understanding one another and promoted the hopes of creating a machine that
could translate languages, automatically. Not too surprisingly, the goal of
building a language translation machine wasn’t as easy as first assumed.
However, while human languages are filled with chaos and broken rules, the
language of mathematics is not. Language translation machines could be adapted
quite successfully to mathematics, with its unchangeable rules. **Machine
Learning and the Game of Checkers**
--------------------------------------------- Arthur Samuel of IBM developed a
computer program for [playing checkers](https://medium.com/ibm-data-ai/the-
first-of-its-kind-ai-model-samuels-checkers-playing-program-1b712fa4ab96) in
the early 1950s. He completed a number of algorithms that allowed his checker-
playing program to improve and described it as “[machine
learning](https://www.dataversity.net/machine-learning-101-2/)” in 1959. **The
Mark 1 Perceptron Uses Neural Networks**
----------------------------------------------- In 1958, Frank Rosenblatt, of
the Cornell Aeronautical Laboratory, merged Hebb’s algorithmic model of neural
networks with Samuel’s work on machine learning, creating the first artificial
neural network, called the Mark 1 Perceptron. Although language translation
was still a goal, computers were being built primarily for mathematical
purposes (much less chaotic than languages). These huge computers, built with
vacuum tubes, and used as calculators, were not manufactured, but built
individually, as were their software programs. The Perceptron was also unique
because it used software designed for the IBM 704, and established that
similar computers could share standardized software programs. Unfortunately,
the [Mark 1 Perceptron](https://blog.knoldus.com/introduction-to-perceptron-
neural-network/) could not recognize many kinds of basic visual patterns (such
as faces), resulting in broken expectations and cuts to neural network
research and machine learning. **ELIZA Uses Natural Language Programming**
------------------------------------------- In 1966, an MIT computer
scientist, Joseph Weizenbaum, developed ELIZA, which is described as the first
program using NLP. It could identify keywords from the input it received, and
respond with a pre-programmed answer. Weizenbaum was attempting to prove his
assumption that the communications between humans and machines were
fundamentally superficial, but things didn’t work out as planned. To simplify
the experiment and minimize disputes, Weizenbaum developed a program using
“[active listening](https://www.ccl.org/articles/leading-effectively-
articles/coaching-others-use-active-listening-skills/),” which did not require
a database storing real-world information, but would reflect back a person’s
statements to carry the conversation forward. He was surprised (perhaps even
horrified) that people, including Weizenbaum’s own secretary, described the
computer program as having human-like feelings. Weizenbaum
[wrote](https://www.filfre.net/2011/06/eliza-part-3/): “My secretary, who had
watched me work on the program for many months and therefore surely knew it to
be merely a computer program, started conversing with it. After only a few
interactions with it, she asked me to leave the room.” He [later
added](https://99percentinvisible.org/episode/the-eliza-effect/), “I had not
realized … that extremely short exposures to a relatively simple computer
program could induce powerful delusional thinking in quite normal people.” The
original version of [ELIZA](https://sites.google.com/view/elizagen-org/the-
original-eliza) recently became open-source and is available
[here](https://github.com/codeanticode/eliza). **Machine Learning Carries on
as a Separate Industry**
-------------------------------------------------------- The years between
[1974 and 1980](https://www.dataversity.net/a-brief-history-of-machine-
learning/) are referred to as “the first AI winter.” AI researchers had to
deal with two very basic limitations: small amounts of data storage and
painfully slow processing speeds. Most universities had abandoned neural
network research and a schism developed between AI and machine learning.
Before this schism, ML was used primarily to train artificial intelligence.
However, the machine learning industry, which included several researchers and
technicians, reorganized itself into a separate field. After the schism, the
ML industry shifted its focus to probability theory and statistics, while
continuing to work with neural networks. ML was used to answer phones and
perform a variety of automated tasks. Early development of the first (small)
[language models](http://verbit.ai/understanding-language-models-and-
artificial-intelligence/) was started in the 1980s by IBM, and they were/are
designed to predict the next word in a sentence. Part of their design includes
a “dictionary,” which determines how often certain words occur within the text
the model was trained on. After each word, the algorithm recalculates
statistically what the following word should be. (This limited statistical
model does not support the creativity offered by ChatGPT.) **NLP Is combined
with Machine Learning and Research Funding Returns**
---------------------------------------------------------------------- By the
late 1980s, computational power had increased significantly. Additionally,
machine algorithms had improved and a revolution in natural language
processing came about. It was the result of both the steady increase of
computational power and the shift to machine learning algorithms. (Prior to
the 1980s, most NLP systems used complicated, “handwritten” rules.) During the
1990s, the use of statistical models for NLP analyses increased dramatically
because of their speed and the tremendous flow of text moving through the
internet. **The World Wide Web Provides a Massive Source of Data**
-------------------------------------------------------- Tim Berners-Lee
thought of the World Wide Web (WWW) in 1989, and it was made available to the
public in 1991. The World Wide Web makes it possible for large library models
to access massive amounts of data for research. The creation of the World Wide
Web made the internet searchable and provided large language models with
access to massive amounts of information. The [World Wide
Web](https://businessdegrees.uab.edu/blog/internet-vs-world-wide-web-whats-
the-difference/) offers a platform to create, store, locate, and share
information on a variety of topics. During the mid-1990s, the WWW initiated
new levels of use on the internet, promoting interest in online shopping and
what was called “surfing” the internet. **GPUs and Large Language Models**
---------------------------------- Large language models require complex
training, which includes the use of huge amounts of data containing billions
of words and phrases. Training large language models can be described as
training the individual pieces of a massive jigsaw puzzle, with each puzzle
piece representing a portion of the LLM’s understanding. GPUs ([graphics
processing units](https://www.heavy.ai/technical-glossary/cpu-vs-gpu)) provide
a solution to these problems. A GPU is an electronic circuit that was
originally designed to accelerate the processing of computer graphics and
images. GPUs can process several pieces of data simultaneously, which makes
them extremely useful for machine learning, gaming applications, video
editing, and 3D graphics. As the GPU’s memory capacity and speed increased,
they played a significant role in developing sophisticated language models.
**Deep Learning and Large Language Models**
------------------------------------------- In the 1990s, the arrival of deep
learning supported even more advanced language models. A large language model
is a very large deep learning model that is pre-trained on massive amounts of
data. [Deep learning](https://www.dataversity.net/brief-history-deep-
learning/) is a form of machine learning, which is also a neural network, but
with additional layers. In 2011, deep learning began becoming popular. By
2018, deep learning algorithms were being used in every industry, from
photography to online detail. Some of the ways deep learning applications were
used include Apple’s Siri, automated drug design, and NLP for sentiment
analysis. **The Generative Adversarial Neural Network**
---------------------------------------------- In 2014, Ian Goodfellow
introduced the [Generative Adversarial Neural
Network](https://machinelearningmastery.com/what-are-generative-adversarial-
networks-gans/) (a concept prompted by a conversation with friends while at a
bar). The design uses two neural networks, which play against one another in a
game. The game’s goal is for one of the networks to imitate a photo, tricking
the opposing network into believing the imitation is real. The opposing
network is looking for flaws – evidence the photo is not real. The game
continues to be played until the photo is so close to perfect it tricks its
opponent. **Large Language Models Support Smarter AI**
-------------------------------------------- Near the end of 2022, OpenAI
released ChatGPT and changed the world of AI, dramatically. They offered a
powerful new chatbot capable of communicating in normal, human-like English,
and able to complete a wide range of tasks, including developing new software
and writing speeches. [Generative AI](https://www.dataversity.net/how-
generative-ai-will-transform-business/), with the support of large language
models, produced a new level of intelligent behavior in chatbots. OpenAI’s
“smarter chatbots” quickly became a powerful tool useful for research, good
writing, and generating realistic images or videos. A design for large
language models used for the new chatbots, called OpenChatKit, was [open-
sourced](https://openchatkit.net/) on March 10, 2023, by Together Computer.
_Image used under license from Shutterstock_


Extracted page content:
 Title: Reinforcement Learning in Large Language Models: A Technical Overview URL Source: https://johndcyber.com/reinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f Published Time: 2024-09-25T16:24:07.897Z Markdown Content: Reinforcement Learning in Large Language Models: A Technical Overview | by John D Cyber | Medium =============== [Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F40dca7917a0f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---top_nav_layout_nav-----------------------------------------) [Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) [Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) [](https://medium.com/?source=---top_nav_layout_nav-----------------------------------------) [Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------) [](https://medium.com/search?source=---top_nav_layout_nav-----------------------------------------) [Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) [Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) ![Image 8](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png) [Mastodon](https://me.dm/@johndcyber) Reinforcement Learning in Large Language Models: A Technical Overview ===================================================================== [![Image 9: John D Cyber](https://miro.medium.com/v2/resize:fill:88:88/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---byline--40dca7917a0f---------------------------------------) [John D Cyber](https://johndcyber.com/?source=post_page---byline--40dca7917a0f---------------------------------------) ·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89616265a3a6&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=post_page-89616265a3a6--byline--40dca7917a0f---------------------post_header------------------) 6 min read · Sep 25, 2024 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=---header_actions--40dca7917a0f---------------------clap_footer------------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=---header_actions--40dca7917a0f---------------------bookmark_footer------------------) [Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=---header_actions--40dca7917a0f---------------------post_audio_button------------------) Share ![Image 10](https://miro.medium.com/v2/resize:fit:700/0*ufT2td3ymj4SxspZ.png) Introduction ============ Reinforcement learning (RL) is an area of machine learning where an agent learns to make decisions by interacting with an environment. The agent is rewarded for favorable actions and penalized for unfavorable ones. RL is increasingly being applied to large language models (LLMs) to improve their efficiency, accuracy, and adaptability in various tasks.LLMs like GPT-4 have transformed natural language processing (NLP), but they still face limitations, particularly in decision-making tasks. Reinforcement learning can help bridge this gap by allowing models to optimize their behavior based on rewards. Companies like TikTok, Spotify, and Google are leveraging RL to personalize user experiences, recommending content that aligns with user preferences.This article provides an in-depth technical explanation of how reinforcement learning is applied to LLMs, examples of tech companies utilizing this technique, and steps to deploy RL models on cloud platforms. ![Image 11](https://miro.medium.com/v2/resize:fit:700/0*37JcuN1PS3S5n02e) Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) Key Components of Reinforcement Learning ======================================== 1. Agent: The decision-maker, such as a model that interacts with the environment. 2. Environment The space where the agent operates. For LLMs, the environment might be text data or user interactions. 3. State: The current situation the agent is in. 4. Action: Choices the agent can make at any given state. 5. Reward: Feedback given to the agent based on its actions. 6. Policy: The strategy the agent follows to decide its next action. 7. Value Function: Evaluates the long-term rewards for taking a certain action. Reinforcement Learning in Large Language Models =============================================== ![Image 12](https://miro.medium.com/v2/resize:fit:700/0*Ey7OHIOgVepkOiJu) Photo by [Google DeepMind](https://unsplash.com/@googledeepmind?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) Traditional LLMs generate text based on maximum likelihood estimation (MLE), which tries to minimize prediction errors. However, this often leads to suboptimal performance in tasks requiring long-term planning, such as dialogue systems or content recommendations. RL is introduced in this context to enable LLMs to optimize their output based on reward feedback, which can be either human-labeled data or user engagement metrics (clicks, time spent, etc.). How RL is Applied to LLMs ========================= ![Image 13](https://miro.medium.com/v2/resize:fit:700/0*kx0m7hWQH-C3ryiw) Photo by [Jona](https://unsplash.com/@6690img?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) * Fine-tuning with RL: LLMs are initially trained using unsupervised learning on large corpora of text. Then, they are fine-tuned using reinforcement learning to adapt the model to specific tasks like content recommendation, conversation flow, or code generation. * Reward Models: In applications like [TikTok’s recommendation system](https://linkedin.com/pulse/ai-behind-tiktoks-addictive-algorithm-simple-alexander-stahl/) or ChatGPT’s alignment, a reward model is built that assigns scores to each generated output. The LLM is trained to maximize these rewards over time, resulting in improved user engagement or accuracy. * Exploration vs. Exploitation: LLMs face the classic RL dilemma — whether to explore new potential solutions or exploit known, high-reward outcomes. This balance is key to generating creative or optimized solutions. Examples of Reinforcement Learning in Tech ========================================== 1\\. [TikTok’s Personalized Recommendations](https://www.linkedin.com/pulse/impact-ai-tiktok-how-algorithmic-continue-user-experience-lynch-eh5he/) TikTok uses reinforcement learning to recommend content to users. The app tracks user interactions such as likes, shares, and watch time, then uses RL to personalize the feed. The agent (recommendation model) receives a reward for each positive user interaction and uses that information to curate the next set of content. This continuous cycle helps TikTok provide increasingly relevant videos to users. 2\\. [Spotify’s Music Recommendations](https://www.spotify.com/us/safetyandprivacy/understanding-recommendations) Spotify leverages RL to optimize its Discover Weekly playlist. The model takes into account the music a user listens to, skips, or saves, and tries to recommend songs that maximize user satisfaction. The RL agent receives feedback on the success of its recommendations based on how much time users spend listening or saving songs to playlists. 3\\. Google Search Google has incorporated reinforcement learning into its search algorithms to optimize the search results users are most likely to click on. The RL agent is rewarded when a user interacts with a search result (e.g., clicks, reads) and penalized for poor interactions like rapid bouncing off the site. Technical Dive: Reinforcement Learning in LLMs ============================================== ![Image 14](https://miro.medium.com/v2/resize:fit:700/0*3jNmjK2ITIsTFflQ) Photo by [Scott Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 1. Proximal Policy Optimization (PPO) Proximal Policy Optimization (PPO) is one of the most common RL algorithms used in fine-tuning LLMs. It’s effective for handling high-dimensional tasks like natural language generation, where the agent optimizes its policy by interacting with a reward model. In LLMs, PPO is used for language tasks such as summarization, translation, and dialogue generation. For example, OpenAI fine-tuned GPT-3 using PPO in reinforcement learning from human feedback (RLHF), where the agent learned to generate responses that aligned with human preferences. 2\\. Reinforcement Learning with Human Feedback (RLHF) RLHF is a specific application of RL where human-labeled data helps guide the reward model. ChatGPT is a good example of this approach. The model generates responses, and human annotators provide feedback on which responses are preferable. Over time, the model learns to optimize its output to match human preferences more closely. 3\\. Actor-Critic Methods The Actor-Critic method combines two neural networks: one that takes actions (the Actor) and one that evaluates the actions (the Critic). The Actor decides what to do, and the Critic provides feedback on whether that action was good or bad. In LLMs, this method is used to ensure the model generates high-quality content that is contextually relevant and engaging. ![Image 15](https://miro.medium.com/v2/resize:fit:700/0*fJUA6ObLgxNv-1nG) Photo by [Joel Rivera-Camacho](https://unsplash.com/@actuallyjoel?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) Deploying Reinforcement Learning Models on Cloud Providers ========================================================== Here’s how you can deploy your own reinforcement learning models using a cloud provider such as AWS, Azure, or Google Cloud. Steps to Deploy on AWS ---------------------- **Set up an Environment:**Use Amazon SageMaker to set up your machine learning environment. SageMaker supports various frameworks for RL, including TensorFlow and PyTorch.For example, you can use SageMaker RL for training reinforcement learning models with deep neural networks. **Train the Model:**Create an RL environment using OpenAI Gym or your custom environment.Train your model using RL algorithms like PPO, Deep Q-Learning (DQN), or Actor-Critic. SageMaker provides pre-built algorithms that you can plug into your training process. **Deploy the Model:**Deploy your model on an endpoint for inference using SageMaker.Monitor the agent’s performance in real-time by setting up Amazon CloudWatch to track key metrics like rewards and losses. Steps to Deploy on Google Cloud ------------------------------- **Use AI Platform:**Google Cloud’s AI Platform supports reinforcement learning libraries like TF-Agents (built on TensorFlow) and Ray RLlib for scalable RL.You can set up a training pipeline in Vertex AI and leverage Google Kubernetes Engine (GKE) for distributed RL training. **Hyperparameter Tuning:**Google Cloud provides hyperparameter tuning via HyperTune, allowing you to optimize your RL model’s performance. **Deploy the Model:**After training, deploy your model using Vertex AI Model service to make it available for production use. Steps to Deploy on Azure ------------------------ **Azure Machine Learning:**Use Azure Machine Learning Studio to create, train, and deploy reinforcement learning models.Azure offers pre-configured environments for RL training with popular libraries like OpenAI Gym, Ray, and RLlib. **Distributed Training:**Leverage Azure Kubernetes Service (AKS) for distributed RL training across multiple agents. **Monitoring and Scaling:**Use Azure Monitor to track the performance of your model and scale resources up or down as needed. Summary of Key Points ===================== Reinforcement learning (RL) enhances LLMs by allowing them to optimize their behavior based on rewards, improving long-term decision-making. \\- TikTok, Spotify, and Google are leveraging RL to enhance user experiences through personalized recommendations. \\- Common RL algorithms applied to LLMs include PPO, Actor-Critic methods, and RLHF. \\- RL can be deployed on cloud platforms like AWS, Google Cloud, and Azure using built-in services and libraries, such as Amazon SageMaker, Google AI Platform, and Azure Machine Learning. \\- Proximal Policy Optimization (PPO) and RLHF are key techniques for training RL-enhanced language models. References ========== OpenAI: Reinforcement Learning with Human Feedback. Available at: \\[OpenAI Blog\\](https://openai.com/research/reinforcement-learning-human-feedback) TikTok’s Recommendation System: How Reinforcement Learning Drives User Engagement. Available at: \\[Medium\\](https://medium.com/@tiktok/recommendation) Spotify’s Discover Weekly: Leveraging Reinforcement Learning for Personalized Music Recommendations. Available at: \\[Spotify Blog\\](https://newsroom.spotify.com/tech) AWS SageMaker RL: Documentation and Training Guide. Available at: \\[AWS\\](https://aws.amazon.com/sagemaker/) ![Image 16](https://miro.medium.com/v2/da:true/resize:fit:0/5c50caa54067fd622d2f0fac18392213bf92f6e2fae89b691e62bceb40885e74) Sign up to discover human stories that deepen your understanding of the world. ------------------------------------------------------------------------------ Free ---- Distraction-free reading. No ads. Organize your knowledge with lists and highlights. Tell your story. Find your audience. [Sign up for free](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=---post_footer_upsell--40dca7917a0f---------------------lo_non_moc_upsell------------------) Membership ---------- Read member-only stories Support writers you read most Earn money for your writing Listen to audio narrations Read offline with the Medium app [Try for $5/month](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplans&source=---post_footer_upsell--40dca7917a0f---------------------lo_non_moc_upsell------------------) [Reinforcement Learning](https://medium.com/tag/reinforcement-learning?source=post_page-----40dca7917a0f---------------------------------------) [Llm](https://medium.com/tag/llm?source=post_page-----40dca7917a0f---------------------------------------) [Large Language Models](https://medium.com/tag/large-language-models?source=post_page-----40dca7917a0f---------------------------------------) [AI](https://medium.com/tag/ai?source=post_page-----40dca7917a0f---------------------------------------) [Technical Analysis](https://medium.com/tag/technical-analysis?source=post_page-----40dca7917a0f---------------------------------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=---footer_actions--40dca7917a0f---------------------clap_footer------------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=---footer_actions--40dca7917a0f---------------------clap_footer------------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40dca7917a0f&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=---footer_actions--40dca7917a0f---------------------bookmark_footer------------------) [![Image 17: John D Cyber](https://miro.medium.com/v2/resize:fill:96:96/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---post_author_info--40dca7917a0f---------------------------------------) [![Image 18: John D Cyber](https://miro.medium.com/v2/resize:fill:128:128/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---post_author_info--40dca7917a0f---------------------------------------) [Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89616265a3a6&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=post_page-89616265a3a6--post_author_info--40dca7917a0f---------------------follow_profile------------------) [Written by John D Cyber -----------------------](https://johndcyber.com/?source=post_page---post_author_info--40dca7917a0f---------------------------------------) [269 Followers](https://johndcyber.com/followers?source=post_page---post_author_info--40dca7917a0f---------------------------------------) ·[279 Following](https://johndcyber.com/following?source=post_page---post_author_info--40dca7917a0f---------------------------------------) Experienced Sr. Red Team Engineer with demonstrated skills in DevOps, CICD automation, Cloud Security, Information Security, AWS, Azure, GCP and compliance. [Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89616265a3a6&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&user=John+D+Cyber&userId=89616265a3a6&source=post_page-89616265a3a6--post_author_info--40dca7917a0f---------------------follow_profile------------------) No responses yet ---------------- [](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--40dca7917a0f---------------------------------------) ![Image 19](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png) Write a response [What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Freinforcement-learning-in-large-language-models-a-technical-overview-40dca7917a0f&source=---post_responses--40dca7917a0f---------------------respond_sidebar------------------) Cancel Respond Also publish to my profile More from John D Cyber ---------------------- ![Image 20: Windows .cpl & .msc Commands Cheat sheet](https://miro.medium.com/v2/resize:fit:679/0*8RGsP17akwqM0mRD) [![Image 21: John D Cyber](https://miro.medium.com/v2/resize:fill:20:20/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----0---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [John D Cyber](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----0---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [Windows .cpl & .msc Commands Cheat sheet ---------------------------------------- ### Windows includes a variety of Control Panel (.cpl) and Microsoft Management Console (.msc) shortcuts that allow users to quickly access…](https://johndcyber.com/windows-cpl-msc-commands-cheat-sheet-63777515c109?source=post_page---author_recirc--40dca7917a0f----0---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) Feb 27, 2024 [](https://johndcyber.com/windows-cpl-msc-commands-cheat-sheet-63777515c109?source=post_page---author_recirc--40dca7917a0f----0---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63777515c109&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Fwindows-cpl-msc-commands-cheat-sheet-63777515c109&source=---author_recirc--40dca7917a0f----0-----------------bookmark_preview----1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) ![Image 22: Windows CMD cheat sheet for Windows Administration and Security](https://miro.medium.com/v2/resize:fit:679/1*8aFhMQ7XsV2zwyKliOPshg.jpeg) [![Image 23: John D Cyber](https://miro.medium.com/v2/resize:fill:20:20/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----1---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [John D Cyber](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----1---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [Windows CMD cheat sheet for Windows Administration and Security --------------------------------------------------------------- ### Windows Command Prompt (CMD) cheat sheet tailored for both new users and administrators. This guide covers basic navigation commands, file…](https://johndcyber.com/windows-cmd-cheat-sheet-aa218b279b25?source=post_page---author_recirc--40dca7917a0f----1---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) Feb 28, 2024 [](https://johndcyber.com/windows-cmd-cheat-sheet-aa218b279b25?source=post_page---author_recirc--40dca7917a0f----1---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa218b279b25&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Fwindows-cmd-cheat-sheet-aa218b279b25&source=---author_recirc--40dca7917a0f----1-----------------bookmark_preview----1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) ![Image 24: KQL WINDOWS THREAT HUNTING WITH JohnDCyber](https://miro.medium.com/v2/resize:fit:679/0*kUKTogzu_ZEFE9lf) [![Image 25: John D Cyber](https://miro.medium.com/v2/resize:fill:20:20/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----2---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [John D Cyber](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----2---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [KQL WINDOWS THREAT HUNTING WITH JohnDCyber ------------------------------------------ ### Explore a collection of KQL queries crafted for dynamic threat hunting across a diverse range of topics, techniques, and use cases!](https://johndcyber.com/kql-windows-threat-hunting-with-johndcyber-13041a05c64a?source=post_page---author_recirc--40dca7917a0f----2---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) Jul 11, 2024 [1](https://johndcyber.com/kql-windows-threat-hunting-with-johndcyber-13041a05c64a?source=post_page---author_recirc--40dca7917a0f----2---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13041a05c64a&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Fkql-windows-threat-hunting-with-johndcyber-13041a05c64a&source=---author_recirc--40dca7917a0f----2-----------------bookmark_preview----1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) ![Image 26: How to Create a Reverse TCP Shell Windows Executable Using Metasploit](https://miro.medium.com/v2/resize:fit:679/1*uY7AUXjrMEU9kEBhjM7lBQ.jpeg) [![Image 27: John D Cyber](https://miro.medium.com/v2/resize:fill:20:20/1*AgGSx50f8BzuMi-oXZWfww.jpeg)](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----3---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [John D Cyber](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f----3---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [How to Create a Reverse TCP Shell Windows Executable Using Metasploit --------------------------------------------------------------------- ### In this exploit demonstration, I will be creating a malicious payload in the form of windows executable to create a reverse TCP shell.](https://johndcyber.com/how-to-create-a-reverse-tcp-shell-windows-executable-using-metasploit-56d049007047?source=post_page---author_recirc--40dca7917a0f----3---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) Jan 12, 2021 [1](https://johndcyber.com/how-to-create-a-reverse-tcp-shell-windows-executable-using-metasploit-56d049007047?source=post_page---author_recirc--40dca7917a0f----3---------------------1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F56d049007047&operation=register&redirect=https%3A%2F%2Fjohndcyber.com%2Fhow-to-create-a-reverse-tcp-shell-windows-executable-using-metasploit-56d049007047&source=---author_recirc--40dca7917a0f----3-----------------bookmark_preview----1df949a9_9d6f_40dd_8355_18cce783b7d7--------------) [See all from John D Cyber](https://johndcyber.com/?source=post_page---author_recirc--40dca7917a0f---------------------------------------) Recommended from Medium ----------------------- ![Image 28: AI Agents: Frameworks (Part-3)](https://miro.medium.com/v2/resize:fit:679/1*FoTUnv9KopBXMgPHHISy0Q.png) [![Image 29: Vipra Singh](https://miro.medium.com/v2/resize:fill:20:20/1*LDjQS3c-G1gsojOf24ijGg@2x.jpeg)](https://medium.com/@vipra_singh?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Vipra Singh](https://medium.com/@vipra_singh?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [AI Agents: Frameworks (Part-3) ------------------------------ ### Discover AI agents, their design, and real-world applications.](https://medium.com/@vipra_singh/ai-agents-frameworks-part-3-ca8ce33c2f35?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Feb 21 [5](https://medium.com/@vipra_singh/ai-agents-frameworks-part-3-ca8ce33c2f35?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca8ce33c2f35&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40vipra_singh%2Fai-agents-frameworks-part-3-ca8ce33c2f35&source=---read_next_recirc--40dca7917a0f----0-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) ![Image 30: Group Relative Policy Optimization (GRPO) Illustrated Breakdown & Explanation](https://miro.medium.com/v2/resize:fit:679/1*O_CSxm5jvLR1lYolEIWCvg.png) [![Image 31: Towards AI](https://miro.medium.com/v2/resize:fill:20:20/1*JyIThO-cLjlChQLb6kSlVQ.png)](https://pub.towardsai.net/?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) In [Towards AI](https://pub.towardsai.net/?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) by [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Group Relative Policy Optimization (GRPO) Illustrated Breakdown & Explanation ----------------------------------------------------------------------------- ### A simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training](https://ebrahimpichka.medium.com/group-relative-policy-optimization-grpo-illustrated-breakdown-explanation-684e71b8a3f2?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Jan 31 [1](https://ebrahimpichka.medium.com/group-relative-policy-optimization-grpo-illustrated-breakdown-explanation-684e71b8a3f2?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F684e71b8a3f2&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgroup-relative-policy-optimization-grpo-illustrated-breakdown-explanation-684e71b8a3f2&source=---read_next_recirc--40dca7917a0f----1-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Lists ----- [![Image 32](https://miro.medium.com/v2/resize:fill:48:48/1*aVC9jqv3lDdqFeqFCQ7jYA.png) ![Image 33](https://miro.medium.com/v2/da:true/resize:fill:48:48/0*lKL1De1OLal6IICa) ![Image 34](https://miro.medium.com/v2/resize:fill:48:48/1*3SFfRBW78ZqXmeRAULr-SA.png) Natural Language Processing --------------------------- 1967 stories·1610 saves](https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=post_page---read_next_recirc--40dca7917a0f---------------------------------------) [![Image 35](https://miro.medium.com/v2/da:true/resize:fill:48:48/0*M8Jq6btD0YsgaRM1) ![Image 36](https://miro.medium.com/v2/resize:fill:48:48/1*rsp22rKwFDjiwwCcUly56Q.jpeg) ![Image 37](https://miro.medium.com/v2/resize:fill:48:48/1*PNVLDmurJ5LoCjB9Ovdnpw.png) Generative AI Recommended Reading --------------------------------- 52 stories·1678 saves](https://tomsmith585.medium.com/list/generative-ai-recommended-reading-508b0743c247?source=post_page---read_next_recirc--40dca7917a0f---------------------------------------) [![Image 38](https://miro.medium.com/v2/da:true/resize:fill:48:48/0*_eYHSSUS0abUxmDU) ![Image 39](https://miro.medium.com/v2/resize:fill:48:48/1*wXgeNtz5OJ5O9T3c3mQRRw.png) ![Image 40](https://miro.medium.com/v2/resize:fill:48:48/0*tIipcmrInD5UMpQI.png) What is ChatGPT? ---------------- 9 stories·515 saves](https://medium.com/@MediumForTeams/list/what-is-chatgpt-7a5756752f49?source=post_page---read_next_recirc--40dca7917a0f---------------------------------------) [![Image 41: Image by vectorjuice on FreePik](https://miro.medium.com/v2/resize:fill:48:48/0*3OsUtsnlTx9Svm4c.jpg) ![Image 42](https://miro.medium.com/v2/resize:fill:48:48/1*IPZF1hcDWwpPqOz2vL7NxQ.png) ![Image 43](https://miro.medium.com/v2/resize:fill:48:48/1*0fHUKyg3xtpNWpop35PR4g.png) The New Chatbots: ChatGPT, Bard, and Beyond ------------------------------------------- 12 stories·561 saves](https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=post_page---read_next_recirc--40dca7917a0f---------------------------------------) ![Image 44: Reinforcement Learning, Part 1: Introduction and Main Concepts](https://miro.medium.com/v2/resize:fit:679/1*YgvNWHuASseCfP9DO6DpMQ.png) [![Image 45: TDS Archive](https://miro.medium.com/v2/resize:fill:20:20/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://medium.com/towards-data-science?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) In [TDS Archive](https://medium.com/towards-data-science?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) by [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Reinforcement Learning, Part 1: Introduction and Main Concepts -------------------------------------------------------------- ### Making the first step into the world of reinforcement learning](https://medium.com/@slavahead/reinforcement-learning-introduction-and-main-concepts-48ea997c850c?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Apr 9, 2024 [3](https://medium.com/@slavahead/reinforcement-learning-introduction-and-main-concepts-48ea997c850c?source=post_page---read_next_recirc--40dca7917a0f----0---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48ea997c850c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftowards-data-science%2Freinforcement-learning-introduction-and-main-concepts-48ea997c850c&source=---read_next_recirc--40dca7917a0f----0-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) ![Image 46: DeepSeek-R1: RL for LLMs Rethought](https://miro.medium.com/v2/resize:fit:679/0*poU2e6T5kYfd1QYd) [![Image 47: Anna Alexandra Grigoryan](https://miro.medium.com/v2/resize:fill:20:20/1*tuprc6SjRmyzab0Z8KzZ-A@2x.jpeg)](https://thegrigorian.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Anna Alexandra Grigoryan](https://thegrigorian.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [DeepSeek-R1: RL for LLMs Rethought ---------------------------------- ### For years, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) have been the dominant training methods for…](https://thegrigorian.medium.com/deepseek-r1-rl-for-llms-rethought-e148445d4381?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Jan 31 [1](https://thegrigorian.medium.com/deepseek-r1-rl-for-llms-rethought-e148445d4381?source=post_page---read_next_recirc--40dca7917a0f----1---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe148445d4381&operation=register&redirect=https%3A%2F%2Fthegrigorian.medium.com%2Fdeepseek-r1-rl-for-llms-rethought-e148445d4381&source=---read_next_recirc--40dca7917a0f----1-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) ![Image 48: OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python](https://miro.medium.com/v2/resize:fit:679/0*zf05Z-2A_5rXq-VP.png) [![Image 49: Neural pAi](https://miro.medium.com/v2/resize:fill:20:20/1*WMzhCjQjIT1OxUdTZs9TPQ.png)](https://neuralpai.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----2---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Neural pAi](https://neuralpai.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----2---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python ------------------------------------------------------------------------](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--40dca7917a0f----2---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) 2d ago [](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--40dca7917a0f----2---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc889aed0e784&operation=register&redirect=https%3A%2F%2Fneuralpai.medium.com%2Fopenai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784&source=---read_next_recirc--40dca7917a0f----2-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) ![Image 50: LangChain Agents: Building Intelligent AI Assistants with Gemini Integration](https://miro.medium.com/v2/resize:fit:679/1*4kRYKNA6p9GYXJ5InIz08A.png) [![Image 51: Rümeysa Kara](https://miro.medium.com/v2/resize:fill:20:20/1*cxauvD2nzfxhuQRpaoDdUA.jpeg)](https://rumeysakara.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----3---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [Rümeysa Kara](https://rumeysakara.medium.com/?source=post_page---read_next_recirc--40dca7917a0f----3---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [LangChain Agents: Building Intelligent AI Assistants with Gemini Integration ---------------------------------------------------------------------------- ### In the rapidly evolving landscape of artificial intelligence, LangChain has emerged as a powerful framework for developing sophisticated…](https://rumeysakara.medium.com/langchain-agents-building-intelligent-ai-assistants-with-gemini-integration-a8cd93dba142?source=post_page---read_next_recirc--40dca7917a0f----3---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) Feb 2 [](https://rumeysakara.medium.com/langchain-agents-building-intelligent-ai-assistants-with-gemini-integration-a8cd93dba142?source=post_page---read_next_recirc--40dca7917a0f----3---------------------447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8cd93dba142&operation=register&redirect=https%3A%2F%2Frumeysakara.medium.com%2Flangchain-agents-building-intelligent-ai-assistants-with-gemini-integration-a8cd93dba142&source=---read_next_recirc--40dca7917a0f----3-----------------bookmark_preview----447d9c84_bbdd_4fa9_907c_e40e97417169--------------) [See more recommendations](https://medium.com/?source=post_page---read_next_recirc--40dca7917a0f---------------------------------------) [Help](https://help.medium.com/hc/en-us?source=post_page-----40dca7917a0f---------------------------------------) [Status](https://medium.statuspage.io/?source=post_page-----40dca7917a0f---------------------------------------) [About](https://medium.com/about?autoplay=1&source=post_page-----40dca7917a0f---------------------------------------) [Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----40dca7917a0f---------------------------------------) [Press](mailto:pressinquiries@medium.com) [Blog](https://blog.medium.com/?source=post_page-----40dca7917a0f---------------------------------------) [Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----40dca7917a0f---------------------------------------) [Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----40dca7917a0f---------------------------------------) [Text to speech](https://speechify.com/medium?source=post_page-----40dca7917a0f---------------------------------------) [Teams](https://medium.com/business?source=post_page-----40dca7917a0f---------------------------------------)


